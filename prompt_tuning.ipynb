{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are we going to do in the notebook?**\n",
    "We are going to train two different models using two datasets, each with just one pre-trained model from the Bloom family. One model will be trained with a dataset of prompts, while the other will use a dataset of inspirational sentences. We will compare the results for the same question from both models before and after training.\n",
    "\n",
    "Additionally, we'll explore how to load both models with only one copy of the foundational model in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the PEFT Library**\n",
    "This library contains the Hugging Face implementation of various Fine-Tuning techniques, including Prompt Tuning\n",
    "From the transformers library, we import the necessary classes to instantiate the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the model and the tokenizers.**\n",
    "Bloom is one of the smallest and smartest models available for training with the PEFT Library using Prompt Tuning. You can choose any model from the Bloom Family, and I encourage you to try at least two of them to observe the differences.\n",
    "\n",
    "I'm opting for the smallest one to minimize training time and avoid memory issues in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "dataset = \"xsum\"\n",
    "#model_name = \"bigscience/bloomz-560m\"\n",
    "#model_name=\"bigscience/bloom-1b1\"\n",
    "NUM_VIRTUAL_TOKENS = 4\n",
    "NUM_EPOCHS = 6\n",
    "#TEXT = \"I want you to act as a motivational coach. \"\n",
    "TEXT = \"\"\"Barack Obama has endorsed Vice-President Kamala Harris to be the Democratic presidential nominee, ending days of speculation over whether he would support her.\n",
    "Former President Obama and ex-First Lady Michelle Obama said in a joint statement that they believe Ms Harris has the \"vision, the character, and the strength that this critical moment demands\".\n",
    "Mr Obama was reportedly among more than 100 prominent Democrats Ms Harris spoke to after President Joe Biden announced last Sunday he was dropping out of the race.\n",
    "In a statement at the time, Mr Obama praised Mr Biden's exit, but stopped short of endorsing Ms Harris.\n",
    "The US vice-president has already secured the support of a majority of Democratic delegates, setting her on course to become the official nominee at the party convention in August.\n",
    "The Obamas said in Friday's statement that they could not be \"more thrilled to endorse\" Ms Harris. They vowed to do \"everything we can\" to elect her.\n",
    "\"We agree with President Biden,\" said the couple's statement, \"choosing Kamala was one of the best decisions he’s made. She has the resume to prove it.\"\n",
    "They cited her record as California’s attorney general, a US senator and then vice-president.\n",
    "\"But Kamala has more than a resume,\" the statement continued. \"She has the vision, the character, and the strength that this critical moment demands.\n",
    "\"There is no doubt in our mind that Kamala Harris has exactly what it takes to win this election and deliver for the American people.\n",
    "\"At a time when the stakes have never been higher, she gives us all reason to hope.\"\n",
    "The statement was accompanied by a video of Ms Harris taking a phone call from the Obamas in which they pledge their support.\n",
    "\"Oh my goodness,\" says the vice-president in the clip. \"Michelle, Barack, this means so much to me.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#foundational_model = .from_pretrained(\n",
    "foundational_model = AutoModelForSeq2SeqLM.from_pretrained(    \n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference with the pre trained bloom model**\n",
    "If you want to achieve more varied and original generations, uncomment the parameters: temperature, top_p, and do_sample, in model.generate below\n",
    "\n",
    "With the default configuration, the model's responses remain consistent across calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model, inputs, max_new_tokens=2048):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        #temperature=0.2,\n",
    "        #top_p=0.95,\n",
    "        #do_sample=True,\n",
    "        repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=True, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to have two different trained models, I will create two distinct prompts.\n",
    "\n",
    "The first model will be trained with a dataset containing prompts, and the second one with a dataset of motivational sentences.\n",
    "\n",
    "The first model will receive the prompt \"I want you to act as a motivational coach.\" and the second model will receive \"There are two nice things that should matter to you:\"\n",
    "\n",
    "But first, I'm going to collect some results from the model without Fine-Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\AI\\zerocoder\\fine-tuning\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['has endorsed Kamala Harris to be the Democratic presidential nominee. former president Obama and ex-First Lady Michelle Obama said in joint statement that they believe she has the \"vision, character\"']\n"
     ]
    }
   ],
   "source": [
    "input_prompt = tokenizer(TEXT, return_tensors=\"pt\")\n",
    "foundational_outputs_prompt = get_outputs(foundational_model, input_prompt)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both answers are more or less correct. Any of the Bloom models is pre-trained and can generate sentences accurately and sensibly. Let's see if, after training, the responses are either equal or more accurately generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the Datasets**\n",
    "The Datasets useds are:\n",
    "\n",
    "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
    "https://huggingface.co/datasets/Abirate/english_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset_prompt = \"fka/awesome-chatgpt-prompts\"\n",
    "dataset_prompt = \"xsum\"\n",
    "\n",
    "#Create the Dataset to create prompts.\n",
    "data_prompt = load_dataset(dataset_prompt)\n",
    "#data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"summary\"]), batched=True)\n",
    "train_dataset = data_prompt[\"train\"].select(range(5000))\n",
    "validate_dataset = data_prompt[\"validation\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': ['The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.'], 'summary': ['Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.'], 'id': ['35232142'], 'input_ids': [[7433, 18, 413, 2673, 33, 6168, 640, 8, 12580, 17600, 7, 11, 970, 51, 89, 2593, 11, 10987, 32, 1343, 227, 18368, 2953, 57, 16133, 4937, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-Tuning.**\n",
    "PEFT configurations\n",
    "API docs: https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig\n",
    "\n",
    "We can use the same configuration for both models to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "generation_config = PromptTuningConfig(\n",
    "    #task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, #This type indicates the model will generate text.\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
    "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
    "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating two Prompt Tuning Models.**\n",
    "We will create two identical prompt tuning models using the same pre-trained model and the same config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,096 || all params: 60,510,720 || trainable%: 0.0068\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model_prompt = get_peft_model(foundational_model, generation_config)\n",
    "print(peft_model_prompt.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's amazing: did you see the reduction in trainable parameters? We are going to train a 0.001% of the paramaters available.**\n",
    "\n",
    "Now we are going to create the training arguments, and we will use the same configuration in both trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Seq2SeqTrainingArguments\n",
    "def create_training_arguments(path, learning_rate=0.0035, epochs=6):\n",
    "    #training_args = TrainingArguments(\n",
    "    #    output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "    #    use_cpu=True, # This is necessary for CPU clusters.\n",
    "    #    auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically\n",
    "    #    learning_rate= learning_rate, # Higher learning rate than full Fine-Tuning\n",
    "    #    num_train_epochs=epochs\n",
    "    #)\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "        use_cpu=True, # This is necessary for CPU clusters.\n",
    "        auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically\n",
    "        learning_rate= learning_rate, # Higher learning rate than full Fine-Tuning\n",
    "        num_train_epochs=epochs\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "working_dir = \"./prompt_tuning\"\n",
    "\n",
    "#Is best to store the models in separate folders.\n",
    "#Create the name of the directories where to store the models.\n",
    "output_directory_prompt =  os.path.join(working_dir, \"peft_t5_outputs_prompt\")\n",
    "\n",
    "\n",
    "#Just creating the directoris if not exist.\n",
    "if not os.path.exists(working_dir):\n",
    "    os.mkdir(working_dir)\n",
    "if not os.path.exists(output_directory_prompt):\n",
    "    os.mkdir(output_directory_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to indicate the directory containing the model when creating the TrainingArguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_prompt = create_training_arguments(output_directory_prompt, 0.003, NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "import nltk\n",
    "metric = load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d34360ffa54f9d95dd1f9fe13c1918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# Create a data collator that will pad your inputs and labels\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model_prompt)\n",
    "\n",
    "# Ensure your dataset includes decoder_input_ids\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "validate_dataset = validate_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**\n",
    "We will create the trainer Object, one for each model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "def create_trainer(model, training_args, train_dataset):\n",
    "    #trainer = Trainer(\n",
    "    #    model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "    #    args=training_args, #The args for the training.\n",
    "    #    train_dataset=train_dataset, #The dataset used to tyrain the model.\n",
    "    #    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
    "    #)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "        args=training_args, #The args for the training.\n",
    "        train_dataset=train_dataset, #The dataset used to tyrain the model.\n",
    "        eval_dataset=validate_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator, # mlm=False indicates not to use masked language modeling\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a08aed0ed0741c6803149f3983a4f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training first model.\n",
    "trainer_prompt = create_trainer(peft_model_prompt, training_args_prompt, train_dataset)\n",
    "trainer_prompt.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save models**\n",
    "We are going to save the models. These models are ready to be used, as long as we have the pre-trained model from which they were created in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_prompt.model.save_pretrained(output_directory_prompt)\n",
    "#trainer_sentences.model.save_pretrained(output_directory_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference**\n",
    "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_model_prompt = PeftModel.from_pretrained(foundational_model,\n",
    "                                         output_directory_prompt,\n",
    "                                         #device_map='auto',\n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_prompt_outputs = get_outputs(loaded_model_prompt, input_prompt)\n",
    "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
