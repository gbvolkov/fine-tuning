{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9GE2nkpjjBt"
      },
      "source": [
        "Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install datasets\n",
        "%pip install evaluate\n",
        "%pip install transformers[torch]\n",
        "%pip install transformers\n",
        "%pip install rouge-score\n",
        "%pip install nltk\n",
        "%pip install ipywidgets\n",
        "%pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbO4vD2XjYKu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import load_dataset\n",
        "from evaluate import load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWz3407yjnS3"
      },
      "source": [
        "Load dataset and metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ViqqlAljpKs"
      },
      "outputs": [],
      "source": [
        "dataset = \"xsum\"\n",
        "raw_datasets = load_dataset(dataset, trust_remote_code=True)\n",
        "metric = load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY0V2uTHjzyR"
      },
      "source": [
        "Load model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qovi3OrQj3H6"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyxHBjqFj471"
      },
      "source": [
        "Define prompt-tuning parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P73fjtnNj7PV"
      },
      "outputs": [],
      "source": [
        "num_virtual_tokens = 20  # Number of virtual tokens for prompt-tuning\n",
        "initialize_from_vocab = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CprE2fvdj920"
      },
      "source": [
        "Prepare the prompt embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_qEw5Tzu8YN"
      },
      "source": [
        "\n",
        "1. prompt_embeddings = torch.nn.Embedding(num_virtual_tokens, model.config.d_model)\n",
        "This line creates a new embedding layer for the prompt tokens. Let's break it down:torch.nn.Embedding is a lookup table that stores embeddings of a fixed dictionary and size.\n",
        "num_virtual_tokens is the number of tokens in our prompt (set to 20 earlier in the code).\n",
        "model.config.d_model is the dimensionality of the embeddings in the T5 model.\n",
        "\n",
        "So, this creates an embedding layer for our prompt tokens, where each token will have an embedding vector of the same size as the model's regular token embeddings.\n",
        "2. if initialize_from_vocab:\n",
        "This condition checks whether we want to initialize our prompt embeddings from the model's existing vocabulary. This is often a good starting point, as it gives the prompt embeddings some meaningful initial values.\n",
        "3. prompt_embeddings.weight.data = model.shared.weight[:num_virtual_tokens].clone().detach(). If we're initializing from the vocabulary, this line does the following:\n",
        "*   model.shared.weight accesses the shared embedding weights of the T5 model.\n",
        "*   [:num_virtual_tokens] slices the first num_virtual_tokens embeddings from the model's vocabulary\n",
        "*   .clone() creates a copy of these embeddings\n",
        "*   .detach() detaches these embeddings from the original model's computational graph\n",
        "  This initialization step gives our prompt embeddings a starting point based on actual word embeddings from the model's vocabulary, which can help speed up learning.\n",
        "\n",
        "The purpose of these lines is to create a set of learnable embeddings for our prompt tokens. These embeddings will be prepended to the input embeddings of our actual text, allowing the model to learn task-specific information in the form of these \"virtual tokens\".\n",
        "By initializing from the vocabulary, we're giving these prompt embeddings a head start, using the model's existing knowledge of language as encoded in its word embeddings. This can often lead to faster and more stable training compared to random initialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjiFPLZ4kCjz"
      },
      "outputs": [],
      "source": [
        "prompt_embeddings = torch.nn.Embedding(num_virtual_tokens, model.config.d_model)\n",
        "if initialize_from_vocab:\n",
        "    prompt_embeddings.weight.data = model.shared.weight[:num_virtual_tokens].clone().detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN5SEdT_kEXd"
      },
      "source": [
        "Freeze the model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4eMOhbokG3L"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jm6N6q_kIpT"
      },
      "source": [
        "Make prompt embeddings trainable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5X3sE46kkgv"
      },
      "outputs": [],
      "source": [
        "prompt_embeddings.weight.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up device - ADD THIS CODE HERE\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model and prompt embeddings to device\n",
        "model = model.to(device)\n",
        "prompt_embeddings = prompt_embeddings.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ChzWigkmXA"
      },
      "source": [
        "Modify the model's forward pass to include prompt-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNa7hdS15XQO"
      },
      "source": [
        "The concept of model forwarding, often referred to as the \"forward pass\" or simply \"forward,\" is a fundamental aspect of neural network computation. Let's break this down:\n",
        "\n",
        "1. Basic Concept: The forward pass is the process of passing input data through a neural network to get an output. It's called \"forward\" because the data flows from the input layer, through hidden layers (if any), to the output layer.\n",
        "2. In PyTorch: In PyTorch, the forward pass is typically defined in the forward() method of a nn.Module class. This method describes how input data should be transformed to produce the output.\n",
        "3. Computation Graph: During the forward pass, PyTorch builds a computational graph. This graph records all operations performed on the input data, which is crucial for the backward pass (backpropagation) during training.\n",
        "4. Custom Forward Methods: Sometimes, as in our prompt-tuning example, we need to modify the forward pass of an existing model. This allows us to inject custom behavior, like adding prompt embeddings to the input.\n",
        "5. In the Context of Prompt-Tuning: In our code, we modified the forward pass to include the following steps:\n",
        "- Repeat the prompt embeddings for each item in the batch\n",
        "- Concatenate the prompt embeddings with the input embeddings\n",
        "- Adjust the attention mask to account for the added prompt tokens\n",
        "- Call the original forward method with these modified inputs\n",
        "6. Why Modify the Forward Pass: By modifying the forward pass, we can change how the model processes inputs without altering its fundamental architecture. In prompt-tuning, this allows us to prepend learnable prompt embeddings to every input, effectively giving the model additional context for its task.\n",
        "7. Flexibility: Custom forward methods provide great flexibility. They allow us to adapt pre-trained models for new tasks, implement complex architectures, or introduce novel training techniques like prompt-tuning.\n",
        "8. Efficiency: A well-designed forward pass can also improve computational efficiency. For instance, by doing certain computations in the forward pass, we might avoid repetitive calculations during training.\n",
        "\n",
        "In essence, the forward pass defines how data flows through the model, and by customizing it, we can significantly alter the model's behavior without changing its core parameters. This is particularly useful in transfer learning scenarios, where we want to adapt a pre-trained model to a new task with minimal changes to the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoHbraQ0kpP_"
      },
      "outputs": [],
      "source": [
        "# Save the original forward method\n",
        "original_forward = model.forward\n",
        "\n",
        "def model_forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, \n",
        "                  decoder_input_ids=None, decoder_attention_mask=None, labels=None, **kwargs):\n",
        "    # If we already have inputs_embeds, we're in a recursive call. Just add the prompt and return.\n",
        "    if inputs_embeds is not None:\n",
        "        batch_size = inputs_embeds.shape[0]\n",
        "        prompt_embeds = prompt_embeddings.weight.repeat(batch_size, 1, 1)\n",
        "        inputs_embeds = torch.cat([prompt_embeds, inputs_embeds], dim=1)\n",
        "        \n",
        "        if attention_mask is not None:\n",
        "            attention_mask = torch.cat([torch.ones(batch_size, num_virtual_tokens).to(self.device), attention_mask], dim=1)\n",
        "        \n",
        "        return original_forward(inputs_embeds=inputs_embeds, attention_mask=attention_mask,\n",
        "                                decoder_input_ids=decoder_input_ids, \n",
        "                                decoder_attention_mask=decoder_attention_mask,\n",
        "                                labels=labels, **kwargs)\n",
        "\n",
        "    # If we have input_ids, convert them to embeddings\n",
        "    if input_ids is not None:\n",
        "        batch_size = input_ids.shape[0]\n",
        "        inputs_embeds = self.encoder.embed_tokens(input_ids)\n",
        "        prompt_embeds = prompt_embeddings.weight.repeat(batch_size, 1, 1)\n",
        "        inputs_embeds = torch.cat([prompt_embeds, inputs_embeds], dim=1)\n",
        "        \n",
        "        if attention_mask is not None:\n",
        "            attention_mask = torch.cat([torch.ones(batch_size, num_virtual_tokens).to(self.device), attention_mask], dim=1)\n",
        "        \n",
        "        return original_forward(inputs_embeds=inputs_embeds, attention_mask=attention_mask,\n",
        "                                decoder_input_ids=decoder_input_ids, \n",
        "                                decoder_attention_mask=decoder_attention_mask,\n",
        "                                labels=labels, **kwargs)\n",
        "\n",
        "    # If we have neither, raise an error\n",
        "    raise ValueError(\"You have to specify either input_ids or inputs_embeds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtWP7wVFkrs9"
      },
      "source": [
        "Apply the custom forward method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODeBRChVkwMK"
      },
      "outputs": [],
      "source": [
        "model.forward = model_forward.__get__(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyCjV7TMkyK4"
      },
      "source": [
        "Preprocessing function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpPgdofh6MaR"
      },
      "source": [
        "These parameters were adjusted for specific reasons related to optimizing the prompt-tuning process. Let's break down the changes and their rationale:\n",
        "\n",
        "1. max_input_length = 512 (changed from 1024) Reason for change:\n",
        "- Efficiency: Reducing the maximum input length from 1024 to 512 tokens significantly decreases the memory usage and computational time required for each training step.\n",
        "- Dataset characteristics: For many summarization tasks, especially with the XSum dataset, 512 tokens are often sufficient to capture the main content of the input text.\n",
        "- Prompt-tuning focus: Since we're using prompt-tuning, we want to focus on learning the prompt embeddings rather than processing very long sequences\n",
        "- Shorter inputs allow for more training iterations in the same amount of time.\n",
        "- GPU memory constraints: Smaller input lengths allow for larger batch sizes, which can lead to more stable training, especially on GPUs with limited memory.\n",
        "2. max_target_length = 128 (unchanged) Reason for keeping it the same:\n",
        "- Summarization goal: The XSum dataset aims for extreme summarization, typically producing single-sentence summaries. 128 tokens are usually more than enough for this purpose.\n",
        "- Consistency: Keeping the target length the same ensures that our model's output remains consistent with the original task specifications.\n",
        "\n",
        "The adjustment of these parameters, particularly the reduction of max_input_length, serves several purposes:\n",
        "- Faster training: Shorter sequences mean faster forward and backward passes through the network.\n",
        "- Lower memory usage: This allows for larger batch sizes or training on GPUs with less memory.\n",
        "- More iterations: With faster processing, we can potentially run more epochs or process more examples in the same amount of time.\n",
        "- Focus on prompt: By limiting the input size, we place more emphasis on the role of the learned prompt embeddings in guiding the model's behavior.\n",
        "\n",
        "It's worth noting that while these changes can significantly improve training efficiency, they do come with a trade-off. If the original texts in the dataset frequently exceed 512 tokens in length, we might be losing some information. However, for many summarization tasks, and especially for prompt-tuning, this trade-off is often beneficial.\n",
        "\n",
        "If you find that 512 tokens are not sufficient for your specific use case, you can always adjust this parameter. The key is to balance between having enough context for good summaries and maintaining efficient training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1eXvLyblCCi"
      },
      "outputs": [],
      "source": [
        "#why is changed?\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"document\"]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TtJ-riTlSXz"
      },
      "source": [
        "Tokenize datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfEgCQ-GlTNJ"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCZDG8349okY"
      },
      "source": [
        "Compute metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ1rroGM9qx-"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value * 100 for key, value in result.items()}\n",
        "\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJRnVL3TlU8r"
      },
      "source": [
        "Set up training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ij22X2zlWhH"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-prompt-tuned-xsum\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-3,  # Higher learning rate for prompt-tuning\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,  # Increase epochs as we're training fewer parameters\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVMElCj3lZQh"
      },
      "source": [
        "Define data collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IjmixUClbl6"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FftcwjGmldGf"
      },
      "source": [
        "Set up trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEog6GwYlevv"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    #compute_metrics=lambda pred: metric.compute(predictions=pred.predictions, references=pred.label_ids, use_stemmer=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj7BKn0ylgYF"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyD0YmbSljRA"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91r4c2ablkwn"
      },
      "source": [
        "Push the model to the Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkREHksUlnQ7"
      },
      "outputs": [],
      "source": [
        "#trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Assuming you've already trained your model and it's called 'model'\n",
        "# and your tokenizer is called 'tokenizer'\n",
        "\n",
        "def save_model_locally(model, tokenizer, path):\n",
        "    \"\"\"\n",
        "    Save the model and tokenizer to a local directory.\n",
        "    \"\"\"\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "    \n",
        "    # Save the prompt embeddings separately\n",
        "    torch.save(model.prompt_embeddings.state_dict(), f\"{path}/prompt_embeddings.pt\")\n",
        "    print(f\"Model and tokenizer saved to {path}\")\n",
        "\n",
        "def load_model_locally(path):\n",
        "    \"\"\"\n",
        "    Load the model and tokenizer from a local directory.\n",
        "    \"\"\"\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "    \n",
        "    # Load the prompt embeddings\n",
        "    prompt_embeddings = torch.nn.Embedding(20, model.config.d_model)  # Adjust 20 if you used a different number\n",
        "    prompt_embeddings.load_state_dict(torch.load(f\"{path}/prompt_embeddings.pt\"))\n",
        "    \n",
        "    # Attach prompt embeddings to the model\n",
        "    model.prompt_embeddings = prompt_embeddings\n",
        "    \n",
        "    # Recreate the custom forward method\n",
        "    def model_forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
        "        batch_size = input_ids.shape[0]\n",
        "        prompt_embeds = self.prompt_embeddings.weight.repeat(batch_size, 1, 1)\n",
        "        inputs_embeds = self.encoder.embed_tokens(input_ids)\n",
        "        inputs_embeds = torch.cat([prompt_embeds, inputs_embeds], dim=1)\n",
        "        \n",
        "        attention_mask = torch.cat([torch.ones(batch_size, 20).to(self.device), attention_mask], dim=1)\n",
        "        \n",
        "        return self.forward(inputs_embeds=inputs_embeds, attention_mask=attention_mask, **kwargs)\n",
        "\n",
        "    model.forward = model_forward.__get__(model)\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "def summarize_text(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "    Use the model to summarize a given text.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer([text], max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=100, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# 1. Save the model locally\n",
        "save_model_locally(model, tokenizer, \"./my_prompt_tuned_model\")\n",
        "\n",
        "# 2. Load the model from local storage\n",
        "loaded_model, loaded_tokenizer = load_model_locally(\"./my_prompt_tuned_model\")\n",
        "\n",
        "# 3. Run the model on a test sentence\n",
        "test_text = \"\"\"\n",
        "The United Nations has warned that millions of people in South Sudan are facing severe food shortages. \n",
        "The UN's World Food Programme (WFP) says more than seven million people - about two-thirds of the population - are in need of food aid. \n",
        "The agency says the situation has been made worse by flooding, conflict and the economic crisis. \n",
        "South Sudan has been plagued by instability since it gained independence from Sudan in 2011.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize_text(loaded_model, loaded_tokenizer, test_text)\n",
        "print(\"Original text:\")\n",
        "print(test_text)\n",
        "print(\"\\nGenerated summary:\")\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
